This codebase is for a Crew AI Agent System designed to gather and compile real estate data. Its primary function is to process a given property link, extract relevant details, and compile a comprehensive dataset from multiple sources. The system is designed to automate real estate research, improving speed, accuracy, and consistency.

Key Components of the System
	1.	Input
	•	The primary input to the system is a property link (e.g., a URL to a real estate listing or other property-related page).
	•	The link is processed to extract key identifiers and context for further queries.
	2.	Core Processing
	•	Data Extraction Agents
The system uses specialized agents to retrieve information from multiple sources, including but not limited to:
	•	Property listing platforms (e.g., Zillow, Realtor.com)
	•	Government and municipal records (e.g., tax records, zoning information)
	•	Market analysis platforms (e.g., historical sale data, price trends)
	•	Data Compilation
Extracted information is normalized and aggregated into a structured format for analysis and use.
	3.	Output
	•	A detailed report that includes:
	•	Basic property details (e.g., address, type, size, price)
	•	Historical price trends and sale data
	•	Tax and zoning information
	•	Comparable properties and market analysis
	•	Any anomalies or insights flagged by the system

Goals of the System
	•	Accuracy: Ensure the information collected is reliable and up-to-date.
	•	Efficiency: Minimize processing time to deliver results quickly.
	•	Scalability: Handle a high volume of property links simultaneously.
	•	Extensibility: Easily adapt to new data sources or additional types of real estate information.

Rules and Guidelines
	1.	Input Handling
	•	Validate that the provided property link is in an expected format and is accessible.
	•	Handle broken links or unsupported formats gracefully, logging errors for troubleshooting.
	2.	Data Extraction
	•	Prioritize official and trusted sources over user-generated or unverified data.
	•	Ensure the system is compliant with legal and ethical standards for web scraping and API usage.
	3.	Data Compilation
	•	Standardize the format of the extracted data for consistency.
	•	Cross-check data from multiple sources to resolve discrepancies.
	4.	Output Generation
	•	Ensure the final report is clear, detailed, and free of redundant or conflicting information.
	•	Highlight key insights and flag incomplete or questionable data points for review.
	5.	Error Handling
	•	Log any failures during the extraction or compilation process, with sufficient detail for debugging.
	•	Notify users of incomplete reports with an explanation of the missing information.

Technical Notes
	•	Modular Architecture: Each data source or function should be implemented as a separate module or agent to simplify maintenance and updates.
	•	Asynchronous Processing: Use asynchronous operations to handle multiple property links concurrently.
	•	Machine Learning (Optional): Incorporate ML models for predictive insights, anomaly detection, or advanced trend analysis.